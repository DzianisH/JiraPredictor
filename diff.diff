diff --git a/prepare_datasets.py b/prepare_datasets.py
index f6cab79..0b588eb 100644
--- a/prepare_datasets.py
+++ b/prepare_datasets.py
@@ -4,13 +4,15 @@ import numpy as np
 import pandas as pd
 from numpy import random
 from googletrans import Translator
+from tensorflow.python.framework.errors_impl import InternalError
 
 gas_sources = ('data/jira-exports/AXNGA.0-4000.980.csv', 'data/jira-exports/AXNGA.4000-8500.965.csv',
                'data/jira-exports/AXNGA.8500-14000.984.csv', 'data/jira-exports/AXNGA.14000-20000.915.csv',
                'data/jira-exports/AXNGA.20000-.101.csv')
 non_gas_soures = (
     'data/jira-exports/AXNAU.305.csv', 'data/jira-exports/AXNCEE.823.csv', 'data/jira-exports/AXNIN.9.csv',
-    'data/jira-exports/GSGN.66.csv')
+    'data/jira-exports/GSGN.66.csv', 'data/jira-exports/AXON-AXNTH.2.csv', 'data/jira-exports/AXNCN-AXNKR-AXNMY.955.csv',
+    'data/jira-exports/AXNASEAN-AXNCH-AXNTW.982.csv')
 
 print('Joining DataSets...')
 
@@ -31,8 +33,10 @@ def read_jiras(paths) -> pd.DataFrame:
         df_chunk['estimate'] = df_read['Î£ Original Estimate'].values / 3600
         df_chunk['text'] = (df_read['Summary'] + '\n' + df_read['Description']).values
         df = df.append(df_chunk, ignore_index=True)
-
+    
+    df['text'] = df['text'].apply(lambda text: str(text).strip())
     df = df.loc[df['time'] > 0]
+    df = df.loc[df['text'] != '']
     return shuffle(df)
 
 
@@ -52,6 +56,7 @@ def do_translate_chain(text, chain, api):
 def create_with_translation(df: pd.DataFrame):
     translate_chains = (
         ({'src': 'de', 'dest': 'en'},),
+        ({'src': 'zh', 'dest': 'en'},),
         ({'src': 'en', 'dest': 'fr'}, {'src': 'fr', 'dest': 'en'},),
         ({'src': 'en', 'dest': 'it'}, {'src': 'it', 'dest': 'en'},),
         ({'src': 'en', 'dest': 'de'}, {'src': 'de', 'dest': 'en'},),
@@ -62,22 +67,42 @@ def create_with_translation(df: pd.DataFrame):
         ({'src': 'en', 'dest': 'no'}, {'src': 'no', 'dest': 'en'},),
         ({'src': 'en', 'dest': 'sv'}, {'src': 'sv', 'dest': 'en'},),
         ({'src': 'en', 'dest': 'cs'}, {'src': 'cs', 'dest': 'en'},),
+        ({'src': 'en', 'dest': 'pl'}, {'src': 'pl', 'dest': 'en'},),
+        ({'src': 'en', 'dest': 'ru'}, {'src': 'ru', 'dest': 'en'},),
+        ({'src': 'en', 'dest': 'ro'}, {'src': 'ro', 'dest': 'en'},),
+        ({'src': 'en', 'dest': 'el'}, {'src': 'el', 'dest': 'en'},),
+        ({'src': 'en', 'dest': 'zh'}, {'src': 'zh', 'dest': 'en'},),
+        ({'src': 'en', 'dest': 'sk'}, {'src': 'sk', 'dest': 'en'},),
+        ({'src': 'en', 'dest': 'fy'}, {'src': 'fy', 'dest': 'en'},),
+        ({'src': 'en', 'dest': 'ja'}, {'src': 'ja', 'dest': 'en'},),
         ({'src': 'de', 'dest': 'en'}, {'src': 'en', 'dest': 'fr'}, {'src': 'fr', 'dest': 'en'},),
+        ({'src': 'zh', 'dest': 'en'}, {'src': 'en', 'dest': 'fr'}, {'src': 'fr', 'dest': 'en'},),
         ({'src': 'de', 'dest': 'en'}, {'src': 'en', 'dest': 'nl'}, {'src': 'nl', 'dest': 'en'},),
+        ({'src': 'zh', 'dest': 'en'}, {'src': 'en', 'dest': 'nl'}, {'src': 'nl', 'dest': 'en'},),
+        ({'src': 'de', 'dest': 'en'}, {'src': 'en', 'dest': 'sv'}, {'src': 'sv', 'dest': 'en'},),
+        ({'src': 'de', 'dest': 'en'}, {'src': 'en', 'dest': 'it'}, {'src': 'it', 'dest': 'en'},),
+        ({'src': 'de', 'dest': 'en'}, {'src': 'en', 'dest': 'da'}, {'src': 'da', 'dest': 'en'},),
+        ({'src': 'zh', 'dest': 'en'}, {'src': 'en', 'dest': 'da'}, {'src': 'da', 'dest': 'en'},),
+        ({'src': 'de', 'dest': 'en'}, {'src': 'en', 'dest': 'el'}, {'src': 'da', 'dest': 'en'},),
+        ({'src': 'de', 'dest': 'en'}, {'src': 'en', 'dest': 'no'}, {'src': 'no', 'dest': 'en'},),
     )
+	
     df['original'] = True
     df['lang'] = 'en'
     translated_rows = []
     text_pos = df.keys().get_loc('text')
     original_pos = df.keys().get_loc('original')
     lang_pos = df.keys().get_loc('lang')
-    api = Translator(timeout=30)
+    api = Translator(timeout=30, service_urls=[
+      'translate.google.com',
+      'translate.google.co.kr',
+    ])
     for row in df.values:
         row[original_pos] = False
         src_text = row[text_pos]
         translated_texts = {src_text}
         for chain in translate_chains:
-            # print('creating translations for chain', chain)
+            print('creating translations for chain', chain)
             new_text = do_translate_chain(src_text, api=api, chain=chain)
             if not translated_texts.__contains__(new_text):
                 translated_texts.add(new_text)
@@ -89,8 +114,10 @@ def create_with_translation(df: pd.DataFrame):
 
 print('adding translations for non gas')
 df_non_gas = create_with_translation(df_non_gas)
+df_non_gas = shuffle(df_non_gas)
 print('adding translations for gas')
 df_gas = create_with_translation(df_gas)
+df_gas = shuffle(df_gas)
 
 df_gas.to_csv('data/gas-with-translations.csv')
 df_non_gas.to_csv('data/non-gas-with-translations.csv')
@@ -134,7 +161,13 @@ with tf.Graph().as_default():
         for start in range(0, ds.shape[0], step):
             end = start + step
             chunk = ds[start:end]['text'].values
-            chunk = sess.run(embeddings, {sentences: chunk})
+            try:
+                chunk = sess.run(embeddings, {sentences: chunk})
+            except InternalError:
+                print('wtf detected')
+                df = pd.DataFrame(chunk)
+                df.to_csv('wtf.csv')
+                quit()
             embeds = np.append(embeds, chunk, 0)
             print(embeds.shape)
         return {'embeddings': embeds}
diff --git a/workspace_setup.sh b/workspace_setup.sh
index 29dbe84..5adecac 100644
--- a/workspace_setup.sh
+++ b/workspace_setup.sh
@@ -1,8 +1,13 @@
 pip3 install pandas
 pip3 install numpy
-pop3 install tensorflow
-pop3 install tensorflow-hub
+pip3 install tensorflow
+pip3 install tensorflow-hub
 # https://stackoverflow.com/questions/52455774/googletrans-stopped-working-with-error-nonetype-object-has-no-attribute-group
-pip3 install googletrans
+# pip3 install googletrans
+git clone https://github.com/BoseCorp/py-googletrans.git
+cd ./py-googletrans
+python3 setup.py install
+cd ..
 
-export TFHUB_CACHE_DIR models_cache
\ No newline at end of file
+export TFHUB_CACHE_DIR=models_cache
+mkdir $TFHUB_CACHE_DIR
